<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CustomContrast: A Multilevel  Contrastive Perspective For Subject-Driven Text-to-Image Customization.">
  <meta name="keywords" content="Text-to-Image Customization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CustomContrast</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CustomContrast: A Multilevel  Contrastive Perspective For Subject-Driven Text-to-Image Customization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/CN-makers">Nan Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://corleone-huang.github.io/">Mengqi Huang</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ow1jGJkAAAAJ&hl=zh-CN&oi=ao">Zhuowei Chen</a><sup>1</sup><sup>2</sup>,
              </span>
            <span class="author-block">
              <a href="https://github.com/shuoyueli4519/">Yang Zheng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Lei_Zhang54">Lei Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=m-0P8sgAAAAJ">Zhendong Mao</a><sup>1</sup><sup>&#10013</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China, </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>ByteDance, </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>&#10013</sup>Corresponding author.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Corleone-Huang/RealCustomProject"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Uncomment the video element if needed -->
      <!-- 
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/images/teaser.png" type="video/mp4">
      </video> 
      -->
      <h2 class="title is-3 has-text-centered">Motivation</h2>
      <div class="subtitle has-text-centered">
        <img src="./static/images/intro1.png" alt="Interpolate start reference image.">
      </div>
      <div class="content has-text-left">
        <p>
          Comparison with existing perspective. 
          (a) Existing studies learn each subject feature with entangled redundant features 
          (e.g., view, pose), suffering a trade-off between similarity and controllability 
          (redundant and intrinsic features simultaneously overfit or underfit since they are 
          coupled together). 
          (b) In contrast, we rethink it from a cross-differential perspective.  
          By using contrastive learning to ensure intra-consistency (features of the same subject 
          are spatially closer) and inter-distinctiveness (features of different subjects have 
          distinguished differences), our model disentangles the subject intrinsic features from 
          irrelevant features for dual optimization of controllability and similarity.
        </p>
      </div>
    </div>
  </div>
</section>


<!--examples-->
<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2 class="title is-3 is-centered">Show Cases (based on SDXL)</h2>
      </div>

      <div id="results-carousel-face" class="carousel results-carousel">

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case1.jpg"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case2.jpg"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case3.jpg"
                   alt="Puppet."/>
          </div>
        </div>
        
        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case4.jpg"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case5.jpg"
                   alt="Puppet."/>
          </div>
        </div>
        
        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images/slider/show_case6.jpg"
                   alt="Puppet."/>
          </div>
        </div>

      </div>
    </div>
  </div>
</section> -->

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Subject-driven text-to-image (T2I) customization has drawn significant interest in academia 
            and industry. This task enables pre-trained models to generate novel images based on unique subjects.
            Existing studies adopt a self-reconstructive perspective, focusing on capturing all details of a 
            single image, which will misconstrue the specific image's irrelevant attributes (e.g., view, pose, 
            and background) as the subject intrinsic attributes. This misconstruction leads to both overfitting or
            underfitting of irrelevant and intrinsic attributes of the subject, i.e., these attributes are 
            over-represented or under-represented simultaneously, causing a trade-off between similarity and 
            controllability. In this study, we argue an ideal subject representation can be achieved by a 
            cross-differential perspective, i.e., decoupling subject intrinsic attributes from 
            irrelevant attributes via contrastive learning,  which allows the model to focus more on intrinsic 
            attributes through intra-consistency  (features of the same subject are spatially closer) and
            inter-distinctiveness (features of different subjects have distinguished differences). 
            Specifically, we propose CustomContrast, a novel framework, which includes a
            Multilevel Contrastive Learning (MCL) paradigm and a Multimodal Feature Injection (MFI) Encoder. 
            The MCL paradigm is used to extract intrinsic features of subjects from high-level semantics 
            to low-level appearance through crossmodal semantic contrastive learning and multiscale appearance
            contrastive learning.  To facilitate contrastive learning, we introduce the MFI encoder to
            capture cross-modal representations. Extensive experiments show the effectiveness of
            CustomContrast in subject similarity and text controllability.          </p>
          <!-- <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-left"> -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">How does it work?</h2>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
        <img src="./static/images/model_architecture.jpg"
                 alt="Interpolate start reference image."/>
      <div class="content has-text-left">
        <p>
          <b>Core Idea: </b> we propose a <b>cross-differential perspective</b>, i.e., comparing differences between target 
          samples via contrastive learning, which aims to capture each subject's accurate representation from irrelevant representation (e.g., view, pose, and background). 
          This perspective achieves intra-consistency and inter-distinctiveness. Firstly, Intra-consistency is achieved by pulling images of the 
          same subject under different contexts closer, decoupling irrelevant attributes. Secondly, 
          Inter-distinctiveness is ensured by comparing the specific subject with others, thereby further learning the
          fine-grained intrinsic features. These allow the model to focus more on the intrinsic subject attributes 
          than redundant attributes.           
          <br>
          <b>MFI-Encoder: </b> 
          The Multimodal Feature Injection (MFI) Encoder is designed to extract multimodal features (textual and image
          features) to support the implementation of multilevel contrastive learning, which consists of three main components: Visual-Qformer, Textual-Qformer, and the TV Fusion Module. 
          Visual-Qformer and Textual-Qformer are used to extract visual and textual embeddings. The TV Fusion module uses textual embeddings 
          as queries to capture text-related visual embeddings. 

          <br>
          <b>Multilevel Contrastive Learning Paradigm: </b> 
          The Multilevel Contrastive Learning (MCL) paradigm includes CSCL, aligning high-level semantics by contrasting visual and textual embeddings via CLS tokens,  and MACL, 
          which is applied to text embeddings from different cross-attention layers. MACL decouples redundant subject features by aligning positive samples 
          (segmented images of the same subject from various views, positions, and sizes), while preserving relative
           distances by contrasting with other subjects.
        </p>
      </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Comparison To Current Methods</h2>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
        <img src="./static/images/example.jpg"
                 alt="Interpolate start reference image."/>
        <p>
          Qualitative comparison with existing methods. CustomContrast decouples intrinsic features from
           redundant features, enabling flexible text control over complex pose e.g., the cat toy in the first row
            and shape (e.g., cat driving car in the fourth row) transformations. In contrast, other methods
             underperform due to the influence of coupled redundant features.
        </p>
        <img src="./static/images/sd15_supplement.jpg"
        alt="Interpolate start reference image."/>
        <p>
          Qualitative results with existing methods on SD-v1.5. Compared to previous models based on SD-v1.5,
           our method excels in text editability and individual similarity. It performs well even with complex
            shape  (e.g., turning a vase into a cup in the second row) and accessorization modification 
            (e.g., clock with angle wings in the fourth row), where other models underperform.
        </p>
        <img src="./static/images/multi_subject.png"
        alt="Interpolate start reference image."/>
        <p>
          To further demonstrate CustomContrast's capabilities in complex editing, we present qualitative
           results in multi-object. Our model can preserve the multiple subject similarities while achieving
          superior editing capabilities (e.g., only the cat toy wears a hat in the first row; only the cat
           wears glasses in the third row). 
        </p>
        <img src="./static/images/humanface.jpg"
        alt="Interpolate start reference image."/>
        <p>
          Qualitative comparison of human domain generation with existing methods.  Compared with previous work, 
          our model achieves more accurate style transformations (e.g., the second row) and viewpoint changes 
          (e.g., the third row) while preserving ID similarity. This indicates that our model has learned intrinsic
           ID features and successfully disentangled redundant features. 
        </p>

        <img src="static/images/comparison_quatative_sd15.png"
                 alt="Interpolate start reference image."/>
        <p>
          <div class="content has-text-left">
          <b>Quantitative comparisons with SD-v1.5 based methods. </b>
          <br>
          <b>Left :</b>  CustomContrast demonstrates significant improvements in both text controllability and 
          image similarity compared to previous SD-V1.5 methods, with a notable increase in the ImageReward metric 
          from 0.157 to 0.454; 
          <br>
          <b>Right :</b> Performance visualization with existing
           SD-v1.5-based methods shows our model's superior text controllability and subject similarity. 
           The blue line illustrates the dual optimum paradox (controllability and similarity) by adjusting 
           the weight factor of the IP-Adapter.
          </div>
        </p>
        <img src="static/images/sdxl_table.png"
            alt="Interpolate start reference image."/>
        <p>
          <div class="content has-text-left">
          <br>
          <b>Quantitative comparisons</b>  with more advanced base models. Kv2.2 refers to  Kandinsky v2.2. The quantitative comparison for SDXL indicates that
           our method also improves both controllability and similarity, with the CLIP-T  increasing from 0.312 to 0.329.
        
          </div>
        </p>

      </div>
    </div>


    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Visualization</h2>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
        <img src="./static/images/visualization_1.png"
        alt="Interpolate start reference image."/>
        <p>
        <div class="content has-text-left">
          (a) Dog's view examples. (b) T-SNE of ELITE representations. (c)  T-SNE of our representations, 
          where 2nd represents the second cross attention, and so on. Ideal representations should achieve  
          intra-consistency (i.e., points with the same shape should be spatially closer in the figure). 
          ELITE is influenced by different views,  with view features coupled into the representations.  
          However,  CustomContrast  decouples view features from the intrinsic features,  preserving 
          relative distances between samples.
        </div>
        </p>
        <!-- <img src="./static/images/attention.jpg"
                 alt="Interpolate start reference image."/>
        <p>
          <div class="content has-text-left">
          Illustration 
          </div>
        </p> -->

      </div>
    </div>


  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> -->
    <!-- </div> -->
    <!--/ Matting. -->

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
